services:
  rag:
    platform: linux/amd64
    image: localhost/rag-server-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/rag-server/Dockerfile
    env_file: .env
    environment:
      DEV: ${DEV}
      DEBUG: ${DEBUG:-0}
      MCP_DISABLE_AUTH: ${MCP_DISABLE_AUTH}
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8050
    ports: 
      - "8050:8050"
    volumes: 
      - ./resources:/app/resources:ro
    networks: 
      - freva-gpt
  code:
    platform: linux/amd64
    image: localhost/code-interpreter-server-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/code-interpreter-server/Dockerfile
    env_file: .env
    environment:
      DEV: ${DEV}
      DEBUG: ${DEBUG:-0}
      MCP_DISABLE_AUTH: ${MCP_DISABLE_AUTH}
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8051
    ports: 
      - "8051:8051"
    networks: 
      - freva-gpt
  litellm:
    platform: linux/amd64
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${INSTANCE_NAME}
    volumes: 
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command: "--config=/app/config.yaml"
    env_file: .env 
    ports: 
      - "4000:4000" # Expose LiteLLM port to the host machine for direct access if needed, e.g. local dev
    networks: 
      - freva-gpt
  ollama: # Ollama is only for the backend and not accessible from the outside. Port 11434 is not exposed.
    image: ollama
    build:
      context: .
      dockerfile: docker/Dockerfile.ollama
    hostname: ollama-instance-${INSTANCE_NAME}
    networks: 
      - freva-gpt

networks:
  freva-gpt:
    driver: bridge
# The subnet is not set to avoid conflicts with other potentially running networks on the same machine.
# Note: Named networks by default aren't shared between different docker-compose projects, even if they have the same name, as long as they are in separate directories.

