services:
  freva-gpt-backend:
    image: freva-gpt-backend-${FREVAGPT_INSTANCE_NAME:?Instance name must be set in the .env file to differentiate between instances}
    build:
      context: .
      dockerfile: docker/freva-gpt-backend/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_DEV: ${FREVAGPT_DEV}
      FREVAGPT_DEBUG: ${FREVAGPT_DEBUG:-0}
    hostname: freva-gpt-backend-instance-${FREVAGPT_INSTANCE_NAME}
    ports:
      - "${FREVAGPT_TARGET_PORT:-8502}:${FREVAGPT_BACKEND_PORT:-8502}" # If either is unset, the default value will be used.
      - "8600:${FREVAGPT_BACKEND_PORT:-8502}" # This is to listen to the backend port without and frontend. localhost:8600/docs give FastAPI docs
      - "${FREVAGPT_DEBUG_PORT:-5678}:${FREVAGPT_DEBUG_PORT:-5678}" # Port for debugpy
    volumes: 
      - ./cache/:/app/cache
      - ./logs/:/app/logs
    networks: 
      - freva-gpt
  rag:
    image: rag-server-${FREVAGPT_INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/rag-server/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_DEV: ${FREVAGPT_DEV}
      FREVAGPT_DEBUG: ${FREVAGPT_DEBUG:-0}
      FREVAGPT_MCP_DISABLE_AUTH: ${FREVAGPT_MCP_DISABLE_AUTH}
      FREVAGPT_MCP_HOST: 0.0.0.0
      FREVAGPT_MCP_PORT: 8050
    volumes: 
      - ./resources:/app/resources:ro
      - ./logs/:/app/logs
    networks: 
      - freva-gpt
  code:
    image: code-server-${FREVAGPT_INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/code-server/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_DEV: ${FREVAGPT_DEV}
      FREVAGPT_DEBUG: ${FREVAGPT_DEBUG:-0}
      FREVAGPT_MCP_DISABLE_AUTH: ${FREVAGPT_MCP_DISABLE_AUTH}
      FREVAGPT_MCP_HOST: 0.0.0.0
      FREVAGPT_MCP_PORT: 8051
    volumes:
      - ./cache/:/app/cache
      - ./logs/:/app/logs
    networks: 
      - freva-gpt
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${FREVAGPT_INSTANCE_NAME}
    volumes: 
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command: "--config=/app/config.yaml"
    env_file: .env 
    ports: 
      - "4000:4000" # Expose LiteLLM port to the host machine for direct access if needed, e.g. local dev
    networks: 
      - freva-gpt
  ollama: # Ollama is only for the backend and not accessible from the outside. Port 11434 is not exposed.
    image: ollama
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile.dev
    hostname: ollama-instance-${FREVAGPT_INSTANCE_NAME}
    networks: 
      - freva-gpt
  mongodb:
    networks:
      - freva-gpt
    image: mongo:latest
    environment:
      MONGO_INITDB_ROOT_USERNAME: mongo
      MONGO_INITDB_ROOT_PASSWORD: secret
    hostname: mongodb
    ports:
      - "27016:27017"
    command: --quiet --logpath /dev/null 

networks:
  freva-gpt:
    driver: bridge
# The subnet is not set to avoid conflicts with other potentially running networks on the same machine.
# Note: Named networks by default aren't shared between different docker-compose projects, even if they have the same name, as long as they are in separate directories.
