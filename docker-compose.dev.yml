services:
  freva-gpt-backend:
    platform: linux/amd64 # We need this, otherwise it defaults to arm64 base image and has mismatch for miniforge installer when built on mac
    image: localhost/freva-gpt-backend-${INSTANCE_NAME:?Instance name must be set in the .env file to differentiate between instances}
    build:
      context: .
      dockerfile: docker/freva-gpt-backend/Dockerfile
    env_file: .env
    environment:
      DEV: ${DEV}
      DEBUG: ${DEBUG:-0}
    hostname: freva-gpt-backend-instance-${INSTANCE_NAME}
    ports:
      - "${TARGET_PORT:-8502}:${BACKEND_PORT:-8502}" # If either is unset, the default value will be used.
      - "8600:${BACKEND_PORT:-8502}" # This is to listen to the backend port without and frontend. localhost:8600/docs give FastAPI docs
      - "${DEBUG_PORT:-5678}:${DEBUG_PORT:-5678}" # Port for debugpy
    volumes: 
      -  ./rw_dir/:/app/rw_dir
      -  ./threads/:/app/threads
    networks: 
      - freva-gpt
  rag:
    platform: linux/amd64
    image: localhost/rag-server-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/rag-server/Dockerfile
    env_file: .env
    environment:
      DEV: ${DEV}
      DEBUG: ${DEBUG:-0}
      MCP_DISABLE_AUTH: ${MCP_DISABLE_AUTH}
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8050
    ports: 
      - "8050:8050"
    volumes: 
      - ./resources:/app/resources:ro
    networks: 
      - freva-gpt
  code:
    platform: linux/amd64
    image: localhost/code-server-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/code-server/Dockerfile
    env_file: .env
    environment:
      DEV: ${DEV}
      DEBUG: ${DEBUG:-0}
      MCP_DISABLE_AUTH: ${MCP_DISABLE_AUTH}
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8051
    ports: 
      - "8051:8051"
    volumes:
      - ./rw_dir/:/app/rw_dir
    networks: 
      - freva-gpt
  litellm:
    platform: linux/amd64
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${INSTANCE_NAME}
    volumes: 
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command: "--config=/app/config.yaml"
    env_file: .env 
    ports: 
      - "4000:4000" # Expose LiteLLM port to the host machine for direct access if needed, e.g. local dev
    networks: 
      - freva-gpt
  ollama: # Ollama is only for the backend and not accessible from the outside. Port 11434 is not exposed.
    image: ollama
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile.dev
    hostname: ollama-instance-${INSTANCE_NAME}
    networks: 
      - freva-gpt

networks:
  freva-gpt:
    driver: bridge
# The subnet is not set to avoid conflicts with other potentially running networks on the same machine.
# Note: Named networks by default aren't shared between different docker-compose projects, even if they have the same name, as long as they are in separate directories.

