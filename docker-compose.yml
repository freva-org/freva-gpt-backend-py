services:
  freva-gpt-backend:
    image: freva-gpt-backend-${FREVAGPT_INSTANCE_NAME:?Instance name must be set in the .env file to differentiate between instances}
    build:
      context: .
      dockerfile: docker/freva-gpt-backend/Dockerfile
    env_file: .env
    hostname: freva-gpt-backend-instance-${FREVAGPT_INSTANCE_NAME}
    ports:
      - "${FREVAGPT_TARGET_PORT:-8502}:${FREVAGPT_BACKEND_PORT:-8502}" # If either is unset, the default value will be used.
      - "${FREVAGPT_DEBUG_PORT:-5678}:${FREVAGPT_DEBUG_PORT:-5678}" # Port for debugpy
    volumes:
      - /work:/work:ro
      - /container/da/freva-gpt-backend-links/${FREVAGPT_INSTANCE_NAME}/logs:/app/logs
      - /scratch/b/b380001/freva-gpt/cache:/app/cache:rw
    networks:
      - freva-gpt
  rag:
    image: rag-server-${FREVAGPT_INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/rag-server/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_MCP_HOST: 0.0.0.0
      FREVAGPT_MCP_PORT: 8050
    volumes:
      - ./resources:/app/resources:ro
      - /container/da/freva-gpt-backend-links/${FREVAGPT_INSTANCE_NAME}/logs:/app/logs
    networks:
      - freva-gpt
  code:
    image: code-server-${FREVAGPT_INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/code-server/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_MCP_HOST: 0.0.0.0
      FREVAGPT_MCP_PORT: 8051
    volumes:
      - /work:/work:ro
      - /scratch/b/b380001/freva-gpt/cache:/app/cache:rw
      - /container/da/freva-gpt-backend-links/${FREVAGPT_INSTANCE_NAME}/logs:/app/logs
    networks:
      - freva-gpt
  web-search:
    image: web-search-server-${FREVAGPT_INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/web-search-server/Dockerfile
    env_file: .env
    environment:
      FREVAGPT_MCP_HOST: 0.0.0.0
      FREVAGPT_MCP_PORT: 8052
    networks: 
      - freva-gpt
    volumes:
      - /container/da/freva-gpt-backend-links/${FREVAGPT_INSTANCE_NAME}/logs:/app/logs
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${FREVAGPT_INSTANCE_NAME}
    volumes:
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command: "--config=/app/config.yaml"
    env_file: .env
    networks:
      - freva-gpt
  ollama: # Ollama is only for the backend and not accessible from the outside. Port 11434 is not exposed.
    image: ollama
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    hostname: ollama-instance-${FREVAGPT_INSTANCE_NAME}
    networks:
      - freva-gpt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  freva-gpt:
    driver: bridge
# The subnet is not set to avoid conflicts with other potentially running networks on the same machine.
# Note: Named networks by default aren't shared between different docker-compose projects, even if they have the same name, as long as they are in separate directories.
